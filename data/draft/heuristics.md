# Taha

Heuristics are designed to find good, approximate solutions to difficult combinatorial problems that otherwise cannot be solved by available optimization algorithms.

A heuristic is a direct search technique that uses favorable rules of thumb to locate improved solutions.

The advantage of heuristics is that they usually find (good) solutions quickly.
The disadvantage is that the quality of the solution (relative to the optimum) is generally unknown.

Early generations of heuristics are based on the greedy search rule that mandates making improvement in the value of the objective function with each search move. The search ends at a local optimum where no further improvements are possible.
In the 1980s, a new generation of metaheuristics sought to improve the quality of the heuristic solutions by allowing the search to escape entrapment at local optima. The realized advantage comes at the expense of increased computations.

10.2 GREEdy (LocAL SEARcH) HEURIStIcS
The main ideas of the greedy heuristic are explained via a single-variable problem.
These ideas are subsequently extended to cover multiple variables.
Define the optimization problem with a solution space S as
Minimize z = F1x2, x  S

## Local search

The local search is a ser


## Greedy heuristic

At iteration $k$:

1. Start from a feasible solution $x_k$;
2. Move to a better solution in the neighborhood of $x_k$.
3. Repeat 2) until no better solution is found.

Solution quality versus computational costs:

- Visiting the *immediate neighborhood* of $x_k$ (e.g., a single 2-Opt move) requires less local search computation at the expense of solution quality.
- Visiting the *expanded neighborhood* (e.g., all 2-Opt moves) may lead to better solution quality at the expense of increased computation costs.

Metaheuristics

## Greedy heuristic

Greedy heuristics aim at iteratively moving to higher quality solutions in the neighborhood until no further improvement can be made. More formally:

> At iteration $k$, the search moves from the current solution $X_k$ to a new solution $X_{k+1} \in N(X_k)$ (the neighborhood of $X_k$) only if the new point improves the value of the objective function $F(X)$.
If no better $X_{k+1}$ can be found in $N(X_k)$ or if a user-specified number of iterations is reached, stop searching.

Visiting the *immediate neighborhood* of $X_k$ (e.g., a couple of 2-Opt moves) requires less local search computation at the expense of solution quality.
Whereas visiting the *expanded neighborhood* (e.g., all 2-Opt moves) may lead to better solution quality at the expense of increased computation costs.

**Problem:** At the end of the process, the search may be entrapped at local optima.

To escape entrapment, metaheuristics are designed to occasionally allow inferior moves, hoping to find better solutions by exploring neighborhoods that look umpromising (at first sight).

Unlike the greedy heuristic, which always terminates when a local optimum is reached, termination of a metaheuristic search can be based on one of the following benchmarks:

1. The number of search iterations exceeds a specified number.
2. The number of iterations since the last best solution exceeds a specified number.
3. The neighborhood associated with the current search point is either empty or
cannot lead to a new viable search move.
4. The quality of the current best solution is acceptable.

# Single-solution based MHs (S-MHs)

Immprove a single solution
Can be viewed as "walks" through neighborhoods or search trajectories  through a problem's search space.

S-MHs ha
Generation phase: a set of candidates $C(s)$ is generated from current solution $s$, typically through local transformations.
Replacement phase: a candidate $s' \in C(s)$ is selected to replace the current solution.

Common search concepts: define *neighborhood* structure and *initial solution*.

    Input: Initial solution s_0.
    Set iteration t = 0.
    Repeat
        Generate candidate solutions C(s_t) (partial or complete neighborhood) from s_t;
        Select a solution from C(s_t) to replace the current solution s_t;
        t = t + 1;
    Until: Stopping criteria satisfied
    Output: Best solution found.

## Neighborhood

> A neighborhood function $N$ is a mapping $N : S \rightarrow 2^S$ that assigns to each solution $s \in S$ a set of neighbor solutions $N(s) \subset S$.
> Neighbor solutions $s' \in N(s)$ are generated through a *move* operators $m$ that performs a small pertubation to the solution $s$.

Locality:

- Weak: Small change - large effect on the solution (extreme case: converge to random)
- Strong: Small change - small effect on the solution (meaningful search in the landscape of the problem)



A solution s′ in the neighborhood of s (s′ ∈ N(S)) is called a neighbor of s. A
neighbor is generated by the application of a move operator m that performs a small
perturbation to the solution s. The main property that must characterize a neighborhood
is locality. Locality is the effect on the solution when performing the move
(perturbation) in the representation.



